<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Andrew Nam]]></title><description><![CDATA[Personal Site and Blog]]></description><link>https://andknam.github.io/</link><generator>GatsbyJS</generator><lastBuildDate>Tue, 09 Nov 2021 03:06:41 GMT</lastBuildDate><item><title><![CDATA[Learning Machine Learning (June 28 - July 5)]]></title><description><![CDATA[Transformers]]></description><link>https://andknam.github.io//posts/learning-ml-july5</link><guid isPermaLink="false">https://andknam.github.io//posts/learning-ml-july5</guid><pubDate>Mon, 12 Jul 2021 10:00:00 GMT</pubDate><content:encoded>&lt;p&gt;In the first learning machine learning post, I dumped my notes on RNNs, CNNs, and Seq2Seq. Getting myself to publish a blog post (even if the post was simply a typed version of my handwritten notes) was a huge help for remembering what I learned. Seeing how useful that was, I’ll be writing down some notes again. This time, instead of just copying my notes into markdown, I’ll be explaining what I’ve learned using more blog-style writing. Let’s begin :)&lt;/p&gt;
&lt;h4 id=&quot;transformers&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#transformers&quot; aria-label=&quot;transformers permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Transformers&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;What is a transformer?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Building off the previous post where we looked at Seq2Seq, the &lt;strong&gt;transformer&lt;/strong&gt; is a deep learning model that also makes use of &lt;strong&gt;attention&lt;/strong&gt; to solve sequence-to-sequence problems. Similar to Seq2Seq, the transformer’s architecture is based on &lt;strong&gt;encoding&lt;/strong&gt; and &lt;strong&gt;decoding&lt;/strong&gt;. The encoding portion consists of a stack of encoders, stacked one of top of the other (and so does the decoding portion). &lt;/p&gt;
&lt;p&gt;To use our transformer, the bottom-most encoder is given an &lt;em&gt;input sequence&lt;/em&gt; (such as a sentence). This encoder considers the sequence one word at a time, and every word is represented as a &lt;em&gt;vector&lt;/em&gt; (size of 512) using an embedding algorithm. Vectors at each position flow through a &lt;strong&gt;self-attention&lt;/strong&gt; layer. The results from this layer are then fed to a &lt;strong&gt;feed-forward neural network&lt;/strong&gt; (FNN). Importantly, every vector flows through its own path in the encoder. For the self-attention layer, there are dependencies between the paths. However, the FNN layer does not have these dependencies, so paths can be executed in parallel (speed boost!). &lt;/p&gt;
&lt;h4 id=&quot;attention&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#attention&quot; aria-label=&quot;attention permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Attention&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;How does the &lt;strong&gt;self attention&lt;/strong&gt; layer work? &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Given a sentence such as, “The animal didn’t cross the street because it was too tired”, our transformer needs a way to know what “it” refers to. Self-attention achieves this by baking the “understanding” of other relevant words into the word currently being processed (look at other positions to improve the word encoding). &lt;/p&gt;
&lt;p&gt;The calculation for self-attention using vectors (size of 64) is the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a &lt;strong&gt;query&lt;/strong&gt;, &lt;strong&gt;key&lt;/strong&gt;, and &lt;strong&gt;value&lt;/strong&gt; vector from the encoder input (multiply the input embedding by matrices trained during the training process)&lt;/li&gt;
&lt;li&gt;Take the &lt;em&gt;dot product&lt;/em&gt; of the query and key vector for each word (this numerical &lt;em&gt;score&lt;/em&gt; determines how much focus to place on other parts of the input sequence when encoding a given word)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Divide&lt;/em&gt; scores by 8 (square root of vector size) —&gt; more stable gradients&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Softmax&lt;/em&gt; the scores&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Multiply&lt;/em&gt; each value vector by the softmax score&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sum&lt;/em&gt; up the weighted value vectors —&gt; result is a vector!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more speed, the self-attention calculations are done with matrices instead of vectors. &lt;/p&gt;
&lt;p&gt;Self-attention done this way works alright. The vectors resulting from self-attention do contain a little bit of the other encodings, but might be dominated by the word itself. To expand the transformer’s ability to focus on different positions in the input sequence, we use &lt;strong&gt;multi-headed attention&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;For multi-headed attention, we maintain &lt;em&gt;separate&lt;/em&gt; query, key, and value vectors for each head (each set of vectors is randomly initialized and projects the input embeddings into different subspaces after training). We follow the previous self-attention procedure (which was &lt;strong&gt;single-headed attention&lt;/strong&gt;), eight different times with eight sets of weight matrices. This gives us eight attention heads. We concatenate the heads together and multiply this vector by a big weight matrix to obtain our final result vector. &lt;/p&gt;
&lt;h4 id=&quot;positional-encoding-and-residuals&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#positional-encoding-and-residuals&quot; aria-label=&quot;positional encoding and residuals permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Positional Encoding and Residuals&lt;/h4&gt;
&lt;p&gt;Something we haven’t considered yet is the &lt;em&gt;order of words&lt;/em&gt; in the input sequence. To do so, we use &lt;strong&gt;positional encoding&lt;/strong&gt; (add a positional encoding vector to our word embedding vector). The hope here is that adding this vector will create meaningful distances between the word embedding vectors when we calculate self-attention. &lt;/p&gt;
&lt;p&gt;Going back to the encoder architecture, each sub-layer in each encoder has a &lt;strong&gt;residual connection&lt;/strong&gt; around it (same goes for decoders). The residual connection around the self-attention layer allows us to add the positionally-encoded input vector with the output of the self-attention layer. &lt;/p&gt;
&lt;p&gt;Also, each sub-layer is followed by a &lt;strong&gt;layer normalization&lt;/strong&gt; step.&lt;/p&gt;
&lt;h4 id=&quot;decoders&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#decoders&quot; aria-label=&quot;decoders permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Decoders&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;What is a decoder? What is the Linear Layer and Softmax Layer?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Decoders function much the same as the encoders do. The decoder input takes in the combined word embedding + positional encoding and bubbles up results just like the encoders. However, the decoder has an additional &lt;strong&gt;encoder-decoder attention layer&lt;/strong&gt;. Similar to multiheaded self-attention, this layer works with query, key, and value matrices. Instead of multiplying the decoder input by a weights matrix, the key and value matrices come from the output of the top encoder, and the query matrix comes from the add/normalize layer below the encoder-decoder attention layer. The decoder’s self-attention layer is also slightly different. The self-attention layer is only allowed to attend to earlier positions in the output sequence (future positions are &lt;strong&gt;masked&lt;/strong&gt; to prevent “cheating” during training).&lt;/p&gt;
&lt;p&gt;The result of decoding will be a vector of floats, one vector for each word. This vector goes through a &lt;strong&gt;Linear layer&lt;/strong&gt; (fully-connected NN) that projects the output of the decoder stack to a &lt;strong&gt;logits vector&lt;/strong&gt;. Assuming our model knows 10,000 words, our logits vector will be 10,000 cells long, each cell representing the score for a different word. &lt;strong&gt;Softmaxing&lt;/strong&gt; this vector turns the scores into probabilities, from which we choose the cell with the highest probability. The word related to the index of this cell is our final output.&lt;/p&gt;
&lt;h4 id=&quot;training&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#training&quot; aria-label=&quot;training permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Training&lt;/h4&gt;
&lt;p&gt;Let’s say we are translating the word “merci” into “thanks”. The expectation for our desired output is to have a &lt;strong&gt;one-hot encoded vector&lt;/strong&gt; where the one-hot encoded index matches the index in our output vocabulary vector. However, since our model is untrained, we will likely have a vector that has arbitrary scores for each cell. To solve this, we can compare the &lt;strong&gt;probability distributions&lt;/strong&gt; of the untrained vector with our desired output (by subtracting one from the other). This will tell us which vector index is supposed to be one-hot encoded. &lt;/p&gt;
&lt;p&gt;Realistically, we will be doing this with an entire sentence and not just a single word. What we want our model to do is to successively output probability distributions where the first probability distribution will be for the first word, the second distribution for the second word, etc. Since we know our model produces one distribution at a time, we can select the word with the highest probability and simply throw away the other probabilties (this is called &lt;strong&gt;greedy decoding&lt;/strong&gt;). We can also do a &lt;strong&gt;beam search&lt;/strong&gt; which involves holding on to some top number of words, running our model Y times, and taking the output with less error based on X hypotheses. X (&lt;em&gt;beam_size&lt;/em&gt; = number of hypotheses kept in memory) and Y (&lt;em&gt;top_beams&lt;/em&gt; = number of total translations) are hyperparameters. &lt;/p&gt;
&lt;h4 id=&quot;end&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#end&quot; aria-label=&quot;end permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;End&lt;/h4&gt;
&lt;p&gt;Thanks for making it this far! I will be writing about BERT and GPT-3 for my next post :)&lt;/p&gt;
&lt;h4 id=&quot;where-to-learn&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#where-to-learn&quot; aria-label=&quot;where to learn permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Where to Learn&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Jay Alammar’s Blog&lt;/a&gt; (again) &lt;br&gt;
&lt;a href=&quot;https://medium.com/analytics-vidhya/masking-in-transformers-self-attention-mechanism-bad3c9ec235c&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Medium article on Masking&lt;/a&gt; &lt;br&gt;
&lt;a href=&quot;https://stats.stackexchange.com/questions/321054/what-are-residual-connections-in-rnns&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;StackExchange Post on Residual Connections&lt;/a&gt; &lt;br&gt;
&lt;a href=&quot;https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Medium Article on Transformers&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Learning Machine Learning (June 14 - June 23)]]></title><description><![CDATA[RNN, Seq2Seq, and CNN]]></description><link>https://andknam.github.io//posts/learning-ml-june23</link><guid isPermaLink="false">https://andknam.github.io//posts/learning-ml-june23</guid><pubDate>Wed, 23 Jun 2021 10:00:00 GMT</pubDate><content:encoded>&lt;p&gt;In my about page, I detail a little bit about my machine learning (ML) involvement. I’ve worked through a good portion of Coursera’s Machine Learning course, read a few chapters from Jurafsky’s Speech and Language Processing textbook, and have learned a lot about the general framework for ML research from assisting natural language processing (NLP) research.&lt;/p&gt;
&lt;p&gt;Despite all that, starting my internship, I realized my understanding of ML and NLP is still &lt;em&gt;pretty poor&lt;/em&gt;. So, I decided to go through a few resources to get my understanding up to speed with my peers. Below, I’ve dumped some notes. &lt;/p&gt;
&lt;h4 id=&quot;recurrent-neural-networks-rnn&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#recurrent-neural-networks-rnn&quot; aria-label=&quot;recurrent neural networks rnn permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks (RNN)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Vanilla neural networks only work with fixed-size inputs and fixed-size outputs. RNNs allow us to have &lt;strong&gt;variable-length sequences&lt;/strong&gt; as both inputs and outputs.&lt;/li&gt;
&lt;li&gt;Examples include machine translation and sentiment analysis&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RNNs are  recurrent because they use the &lt;strong&gt;same set of weights&lt;/strong&gt; for each step&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;weights: W_xh, W_hh, W_hy&lt;/li&gt;
&lt;li&gt;weights = matrices, biases = vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;equations for h_t and y_t:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;h_t = tanh(W_xh * x_t + W_hh * h_(t-1) + b_h)&lt;/li&gt;
&lt;li&gt;y_t = W_hy * h_t + b_y&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One-hot vectors&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;construct vocab of all words —&gt; assign int index to ea word&lt;/li&gt;
&lt;li&gt;the “one” in each one-hot vector is at word’s correspond. index&lt;/li&gt;
&lt;li&gt;serves as the input to forward phase &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Backward phase&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cross-entropy loss: L = -ln(p_c)&lt;/li&gt;
&lt;li&gt;use gradient descent to minimize loss&lt;/li&gt;
&lt;li&gt;to fully calculate gradient of W_xh back propagation through time (BPTT)&lt;/li&gt;
&lt;li&gt;clipping gradient values —&gt; mitigate exploding gradient problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;sequence-to-sequence-seq2seq-models-with-attention&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#sequence-to-sequence-seq2seq-models-with-attention&quot; aria-label=&quot;sequence to sequence seq2seq models with attention permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sequence to Sequence (Seq2Seq) Models (with attention)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Takes a sequence of items (words, letters, feature of imgs) —&gt; outputs another sequence of items&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An encoder processes input —&gt; compiles into a context vector &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input is one word (represented using word embedding) from input sentence + hidden state &lt;/li&gt;
&lt;li&gt;last hidden state of encoder is passed as context —&gt; decoder&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Limitation: Seq2Seq with  no attention —&gt; context vector is bad for long sentences&lt;/li&gt;
&lt;li&gt;Using attention, encoder can pass &lt;strong&gt;ALL&lt;/strong&gt; hidden states to decoder (instead of just one hidden state)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Attention decoder&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;prepare inputs (encoder hidden states, decoder hidden state)&lt;/li&gt;
&lt;li&gt;score each hidden state&lt;/li&gt;
&lt;li&gt;softmax the scores&lt;/li&gt;
&lt;li&gt;multiply ea vector by its softmaxed score (amplifies hidden states with high scores)&lt;/li&gt;
&lt;li&gt;sum up the weighed vectors = context vector&lt;/li&gt;
&lt;li&gt;context vector + hidden state —&gt; feedfoward NN —&gt; output word &lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;convolutional-neural-networks-cnn&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#convolutional-neural-networks-cnn&quot; aria-label=&quot;convolutional neural networks cnn permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks (CNN)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Neural networks that have a convolutional layer (conv layer -&gt; have filters (2d matrices of numbers))&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Steps for filtering (ex: vertical 3x3 Sobel filter)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;overlay filter on input image at some location&lt;/li&gt;
&lt;li&gt;perform element-wise multiplication between vals of filter and correspond. val.s in image&lt;/li&gt;
&lt;li&gt;sum all element-wise products (sum = output val for destination pixel in output image)&lt;/li&gt;
&lt;li&gt;repeat for all locations&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Sobel filters act as &lt;strong&gt;edge detectors&lt;/strong&gt;!&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Padding: add zeros around input image —&gt; output image is now same size as input&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;this is called “same” padding&lt;/li&gt;
&lt;li&gt;no padding = “valid” padding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Primary parameter for conv layer is the &lt;strong&gt;number of filters&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Pooling: reduce size of input by pooling values together (i.e. max, min, avg)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Softmax layer: fully-connected (dense) layer using Softmax funct. as activation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;digit represented by node with highest prob. —&gt; output of CNN &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;where-to-learn&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#where-to-learn&quot; aria-label=&quot;where to learn permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Where to Learn&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://victorzhou.com/series/neural-networks-from-scratch/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Victor Zhou’s Blog&lt;/a&gt; for RNN and CNN&lt;br&gt;
&lt;a href=&quot;https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Jay Alammar’s Blog&lt;/a&gt; for Seq2Seq&lt;br&gt;
&lt;a href=&quot;https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Google’s Paper on Attention&lt;/a&gt; (that I also need to go through)&lt;/p&gt;</content:encoded></item><item><title><![CDATA[First Post]]></title><description><![CDATA[This is my first post to the blog!]]></description><link>https://andknam.github.io//posts/welcome-to-site</link><guid isPermaLink="false">https://andknam.github.io//posts/welcome-to-site</guid><pubDate>Tue, 15 Jun 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;This is my first post on my newly-made corner of the internet! I posted an article to Medium in the past, but haven’t gone around to posting anything since. Now that I’ve started my internship, I think this website will be a great place for me to write some short snippets on all the ML and NLP-related stuff I’m learning. &lt;/p&gt;</content:encoded></item></channel></rss>