<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Andrew Nam]]></title><description><![CDATA[Personal Site and Blog]]></description><link>https://andknam.github.io/</link><generator>GatsbyJS</generator><lastBuildDate>Thu, 24 Jun 2021 01:40:35 GMT</lastBuildDate><item><title><![CDATA[Learning Machine Learning (June 14 - June 23)]]></title><description><![CDATA[RNN, Seq2Seq, and CNN]]></description><link>https://andknam.github.io//posts/learning-ml-june23</link><guid isPermaLink="false">https://andknam.github.io//posts/learning-ml-june23</guid><pubDate>Wed, 23 Jun 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;In my about page, I detail a little bit about my machine learning (ML) involvement. I’ve worked through a good portion of Coursera’s Machine Learning course, read a few chapters from Jurafsky’s Speech and Language Processing textbook, and have learned a lot about the general framework for ML research from assisting natural language processing (NLP) research.&lt;/p&gt;
&lt;p&gt;Despite all that, starting my internship, I realized my understanding of ML and NLP is still &lt;em&gt;pretty poor&lt;/em&gt;. So, I decided to go through a few resources to get my understanding up to speed with my peers. Below, I’ve dumped some notes. &lt;/p&gt;
&lt;h4 id=&quot;recurrent-neural-networks-rnn&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#recurrent-neural-networks-rnn&quot; aria-label=&quot;recurrent neural networks rnn permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Recurrent Neural Networks (RNN)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Vanilla neural networks only work with fixed-size inputs and fixed-size outputs. RNNs allow us to have &lt;strong&gt;variable-length sequences&lt;/strong&gt; as both inputs and outputs.&lt;/li&gt;
&lt;li&gt;Examples include machine translation and sentiment analysis&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RNNs are  recurrent because they use the &lt;strong&gt;same set of weights&lt;/strong&gt; for each step&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;weights: W_xh, W_hh, W_hy&lt;/li&gt;
&lt;li&gt;weights = matrices, biases = vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;equations for h_t and y_t:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;h_t = tanh(W_xh * x_t + W_hh * h_(t-1) + b_h)&lt;/li&gt;
&lt;li&gt;y_t = W_hy * h_t + b_y&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One-hot vectors&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;construct vocab of all words —&gt; assign int index to ea word&lt;/li&gt;
&lt;li&gt;the “one” in each one-hot vector is at word’s correspond. index&lt;/li&gt;
&lt;li&gt;serves as the input to forward phase &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Backward phase&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cross-entropy loss: L = -ln(p_c)&lt;/li&gt;
&lt;li&gt;use gradient descent to minimize loss&lt;/li&gt;
&lt;li&gt;to fully calculate gradient of W_xh back propagation through time (BPTT)&lt;/li&gt;
&lt;li&gt;clipping gradient values —&gt; mitigate exploding gradient problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;sequence-to-sequence-seq2seq-models-with-attention&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#sequence-to-sequence-seq2seq-models-with-attention&quot; aria-label=&quot;sequence to sequence seq2seq models with attention permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Sequence to Sequence (Seq2Seq) Models (with attention)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Takes a sequence of items (words, letters, feature of imgs) —&gt; outputs another sequence of items&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An encoder processes input —&gt; compiles into a context vector &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;input is one word (represented using word embedding) from input sentence + hidden state &lt;/li&gt;
&lt;li&gt;last hidden state of encoder is passed as context —&gt; decoder&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Limitation: Seq2Seq with  no attention —&gt; context vector is bad for long sentences&lt;/li&gt;
&lt;li&gt;Using attention, encoder can pass &lt;strong&gt;ALL&lt;/strong&gt; hidden states to decoder (instead of just one hidden state)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Attention decoder&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;prepare inputs (encoder hidden states, decoder hidden state)&lt;/li&gt;
&lt;li&gt;score each hidden state&lt;/li&gt;
&lt;li&gt;softmax the scores&lt;/li&gt;
&lt;li&gt;multiply ea vector by its softmaxed score (amplifies hidden states with high scores)&lt;/li&gt;
&lt;li&gt;sum up the weighed vectors = context vector&lt;/li&gt;
&lt;li&gt;context vector + hidden state —&gt; feedfoward NN —&gt; output word &lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;convolutional-neural-networks-cnn&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#convolutional-neural-networks-cnn&quot; aria-label=&quot;convolutional neural networks cnn permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Convolutional Neural Networks (CNN)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Neural networks that have a convolutional layer (conv layer -&gt; have filters (2d matrices of numbers))&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Steps for filtering (ex: vertical 3x3 Sobel filter)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;overlay filter on input image at some location&lt;/li&gt;
&lt;li&gt;perform element-wise multiplication between vals of filter and correspond. val.s in image&lt;/li&gt;
&lt;li&gt;sum all element-wise products (sum = output val for destination pixel in output image)&lt;/li&gt;
&lt;li&gt;repeat for all locations&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Sobel filters act as &lt;strong&gt;edge detectors&lt;/strong&gt;!&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Padding: add zeros around input image —&gt; output image is now same size as input&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;this is called “same” padding&lt;/li&gt;
&lt;li&gt;no padding = “valid” padding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Primary parameter for conv layer is the &lt;strong&gt;number of filters&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Pooling: reduce size of input by pooling values together (i.e. max, min, avg)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Softmax layer: fully-connected (dense) layer using Softmax funct. as activation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;digit represented by node with highest prob. —&gt; output of CNN &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;where-to-learn&quot; style=&quot;position:relative;&quot;&gt;&lt;a href=&quot;#where-to-learn&quot; aria-label=&quot;where to learn permalink&quot; class=&quot;anchor before&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; focusable=&quot;false&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewBox=&quot;0 0 16 16&quot; width=&quot;16&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;Where to Learn&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://victorzhou.com/series/neural-networks-from-scratch/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Victor Zhou’s Blog&lt;/a&gt; for RNN and CNN&lt;br&gt;
&lt;a href=&quot;https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Jay Alammar’s Blog&lt;/a&gt; for Seq2Seq&lt;br&gt;
&lt;a href=&quot;https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Google’s Paper on Attention&lt;/a&gt; (that I also need to go through)&lt;/p&gt;</content:encoded></item><item><title><![CDATA[First Post]]></title><description><![CDATA[This is my first post to the blog!]]></description><link>https://andknam.github.io//posts/welcome-to-site</link><guid isPermaLink="false">https://andknam.github.io//posts/welcome-to-site</guid><pubDate>Tue, 15 Jun 2021 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;This is my first post on my newly-made corner of the internet! I posted an article to Medium in the past, but haven’t gone around to posting anything since. Now that I’ve started my internship, I think this website will be a great place for me to write some short snippets on all the ML and NLP-related stuff I’m learning. &lt;/p&gt;</content:encoded></item></channel></rss>