{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/learning-ml-june23","result":{"data":{"markdownRemark":{"id":"dd7947f1-817f-5717-a5c6-15db664b6924","html":"<p>In my about page, I detail a little bit about my machine learning (ML) involvement. I’ve worked through a good portion of Coursera’s Machine Learning course, read a few chapters from Jurafsky’s Speech and Language Processing textbook, and have learned a lot about the general framework for ML research from assisting natural language processing (NLP) research.</p>\n<p>Despite all that, starting my internship, I realized my understanding of ML and NLP is still <em>pretty poor</em>. So, I decided to go through a few resources to get my understanding up to speed with my peers. Below, I’ve dumped some notes. </p>\n<h4 id=\"recurrent-neural-networks-rnn\" style=\"position:relative;\"><a href=\"#recurrent-neural-networks-rnn\" aria-label=\"recurrent neural networks rnn permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Recurrent Neural Networks (RNN)</h4>\n<ul>\n<li>Vanilla neural networks only work with fixed-size inputs and fixed-size outputs. RNNs allow us to have <strong>variable-length sequences</strong> as both inputs and outputs.</li>\n<li>Examples include machine translation and sentiment analysis</li>\n<li>\n<p>RNNs are  recurrent because they use the <strong>same set of weights</strong> for each step</p>\n<ul>\n<li>weights: W_xh, W_hh, W_hy</li>\n<li>weights = matrices, biases = vectors</li>\n</ul>\n</li>\n<li>\n<p>equations for h_t and y_t:</p>\n<ul>\n<li>h_t = tanh(W_xh * x_t + W_hh * h_(t-1) + b_h)</li>\n<li>y_t = W_hy * h_t + b_y</li>\n</ul>\n</li>\n<li>\n<p>One-hot vectors</p>\n<ul>\n<li>construct vocab of all words —> assign int index to ea word</li>\n<li>the “one” in each one-hot vector is at word’s correspond. index</li>\n<li>serves as the input to forward phase </li>\n</ul>\n</li>\n<li>\n<p>Backward phase</p>\n<ul>\n<li>cross-entropy loss: L = -ln(p_c)</li>\n<li>use gradient descent to minimize loss</li>\n<li>to fully calculate gradient of W_xh back propagation through time (BPTT)</li>\n<li>clipping gradient values —> mitigate exploding gradient problem</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"sequence-to-sequence-seq2seq-models-with-attention\" style=\"position:relative;\"><a href=\"#sequence-to-sequence-seq2seq-models-with-attention\" aria-label=\"sequence to sequence seq2seq models with attention permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Sequence to Sequence (Seq2Seq) Models (with attention)</h4>\n<ul>\n<li>Takes a sequence of items (words, letters, feature of imgs) —> outputs another sequence of items</li>\n<li>\n<p>An encoder processes input —> compiles into a context vector </p>\n<ul>\n<li>input is one word (represented using word embedding) from input sentence + hidden state </li>\n<li>last hidden state of encoder is passed as context —> decoder</li>\n</ul>\n</li>\n<li>Limitation: Seq2Seq with  no attention —> context vector is bad for long sentences</li>\n<li>Using attention, encoder can pass <strong>ALL</strong> hidden states to decoder (instead of just one hidden state)</li>\n<li>\n<p>Attention decoder</p>\n<ol>\n<li>prepare inputs (encoder hidden states, decoder hidden state)</li>\n<li>score each hidden state</li>\n<li>softmax the scores</li>\n<li>multiply ea vector by its softmaxed score (amplifies hidden states with high scores)</li>\n<li>sum up the weighed vectors = context vector</li>\n<li>context vector + hidden state —> feedfoward NN —> output word </li>\n</ol>\n</li>\n</ul>\n<h4 id=\"convolutional-neural-networks-cnn\" style=\"position:relative;\"><a href=\"#convolutional-neural-networks-cnn\" aria-label=\"convolutional neural networks cnn permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Convolutional Neural Networks (CNN)</h4>\n<ul>\n<li>Neural networks that have a convolutional layer (conv layer -> have filters (2d matrices of numbers))</li>\n<li>\n<p>Steps for filtering (ex: vertical 3x3 Sobel filter)</p>\n<ol>\n<li>overlay filter on input image at some location</li>\n<li>perform element-wise multiplication between vals of filter and correspond. val.s in image</li>\n<li>sum all element-wise products (sum = output val for destination pixel in output image)</li>\n<li>repeat for all locations</li>\n</ol>\n</li>\n<li>Sobel filters act as <strong>edge detectors</strong>!</li>\n<li>\n<p>Padding: add zeros around input image —> output image is now same size as input</p>\n<ul>\n<li>this is called “same” padding</li>\n<li>no padding = “valid” padding</li>\n</ul>\n</li>\n<li>Primary parameter for conv layer is the <strong>number of filters</strong></li>\n<li>Pooling: reduce size of input by pooling values together (i.e. max, min, avg)</li>\n<li>\n<p>Softmax layer: fully-connected (dense) layer using Softmax funct. as activation</p>\n<ul>\n<li>digit represented by node with highest prob. —> output of CNN </li>\n</ul>\n</li>\n</ul>\n<h4 id=\"where-to-learn\" style=\"position:relative;\"><a href=\"#where-to-learn\" aria-label=\"where to learn permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Where to Learn</h4>\n<p><a href=\"https://victorzhou.com/series/neural-networks-from-scratch/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Victor Zhou’s Blog</a> for RNN and CNN<br>\n<a href=\"https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Jay Alammar’s Blog</a> for Seq2Seq<br>\n<a href=\"https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Google’s Paper on Attention</a> (that I also need to go through)</p>","fields":{"slug":"/posts/learning-ml-june23","tagSlugs":["/tag/machine-learning/","/tag/natural-language-processing/"]},"frontmatter":{"date":"2021-06-23T00:00:00+00:00","description":"RNN, Seq2Seq, and CNN","tags":["Machine Learning","Natural Language Processing"],"title":"Learning Machine Learning (June 14 - June 23)"}}},"pageContext":{"slug":"/posts/learning-ml-june23"}},"staticQueryHashes":["251939775","3698005521","401334301"]}